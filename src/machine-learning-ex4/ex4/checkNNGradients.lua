---
--- Generated by EmmyLua(https://github.com/EmmyLua)
--- Created by shuieryin.
--- DateTime: 12/01/2018 8:54 PM
---

require 'debugInitializeWeights'
require 'computeNumericalGradient'

function checkNNGradients(lambda)
    --CHECKNNGRADIENTS Creates a small neural network to check the
    --backpropagation gradients
    --   CHECKNNGRADIENTS(lambda) Creates a small neural network to check the
    --   backpropagation gradients, it will output the analytical gradients
    --   produced by your backprop code and the numerical gradients (computed
    --   using computeNumericalGradient). These two gradient computations should
    --   result in very similar values.

    lambda = lambda or 0

    local input_layer_size = 3
    local hidden_layer_size = 5
    local num_labels = 3
    local m = 5

    -- We generate some 'random' test data
    local Theta1 = debugInitializeWeights(hidden_layer_size, input_layer_size)
    local Theta2 = debugInitializeWeights(num_labels, hidden_layer_size)
    -- Reusing debugInitializeWeights to generate X
    local X = debugInitializeWeights(m, input_layer_size - 1)
    local y = 1 + torch.range(1, m):mod(num_labels)

    -- Unroll parameters
    local nn_params = torch.reshape(Theta1, Theta1:numel(), 1):cat(torch.reshape(Theta2, Theta2:numel(), 1), 1)

    -- Short hand for cost function
    local costFunc = function(p)
        return nnCostFunction(p, input_layer_size, hidden_layer_size, num_labels, X, y, lambda)
    end
    print('')

    local cost, grad = costFunc(nn_params)
    local numgrad = computeNumericalGradient(costFunc, nn_params)

    -- Visually examine the two gradient computations.  The two columns
    -- you get should be very similar.
    print(numgrad:cat(grad, 2))
    print("The above two columns you get should be very similar.")
    print("Left-Your Numerical Gradient, Right-Analytical Gradient)")

    -- Evaluate the norm of the difference between two solutions.
    -- If you have a correct implementation, and assuming you used EPSILON = 0.0001
    -- in computeNumericalGradient.m, then diff below should be less than 1e-9
    local diff = torch.norm(numgrad - grad) / torch.norm(numgrad + grad)

    print('If your backpropagation implementation is correct, then ')
    print('the relative difference will be small (less than 1e-9).')
    print('Relative Difference:')
    print(diff)
end
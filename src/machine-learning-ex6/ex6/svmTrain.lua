---
--- Generated by EmmyLua(https://github.com/EmmyLua)
--- Created by shuieryin.
--- DateTime: 12/01/2018 9:28 PM
---

function svmTrain(Xsrc, Ysrc, C, kernelFunction, tol, max_passes)
    --SVMTRAIN Trains an SVM classifier using a simplified version of the SMO
    --algorithm.
    --   [model] = SVMTRAIN(X, Y, C, kernelFunction, tol, max_passes) trains an
    --   SVM classifier and returns trained model. X is the matrix of training
    --   examples.  Each row is a training example, and the jth column holds the
    --   jth feature.  Y is a column matrix containing 1 for positive examples
    --   and 0 for negative examples.  C is the standard SVM regularization
    --   parameter.  tol is a tolerance value used for determining equality of
    --   floating point numbers. max_passes controls the number of iterations
    --   over the dataset (without changes to alpha) before the algorithm quits.
    --
    -- Note: This is a simplified version of the SMO algorithm for training
    --       SVMs. In practice, if you want to train an SVM classifier, we
    --       recommend using an optimized package such as:
    --
    --           LIBSVM   (http://www.csie.ntu.edu.tw/~cjlin/libsvm/)
    --           SVMLight (http://svmlight.joachims.org/)
    local X = Xsrc:clone()
    local Y = Ysrc:clone()

    if tol == nil then
        tol = 1e-3
    end

    if max_passes == nil then
        max_passes = 5
    end

    -- Data parameters
    local m = X:size(1)

    -- Map 0 to -1
    Y[Y:eq(0)] = -1

    -- Variables
    local alphas = torch.zeros(m, 1)
    local b = 0
    local E = torch.zeros(m, 1)
    local passes = 0
    local eta = 0
    local L = 0
    local H = 0

    -- Pre-compute the Kernel Matrix since our dataset is small
    -- (in practice, optimized SVM packages that handle large datasets
    --  gracefully will _not_ do this)

    -- We have implemented optimized vectorized version of the Kernels here so
    -- that the svm training will run faster.
    local K
    if kernelFunction.name == 'linearKernel' then
        -- Vectorized computation for the Linear Kernel
        -- This is equivalent to computing the kernel on every pair of examples
        K = X * X:t()
    elseif kernelFunction.name == 'gaussianKernel' then
        -- Vectorized RBF Kernel
        -- This is equivalent to computing the kernel on every pair of examples
        local X2 = torch.pow(X, 2):sum(2)
        K = bsxfun(plus, X2, bsxfun(plus, X2:t(), -2 * (X * X:t())))
        K = torch.cpow(torch.Tensor(K:size()):fill(kernelFunction.gaussian(1, 0)), K)
    else
        -- Pre-compute the Kernel Matrix
        -- The following can be slow due to the lack of vectorization
        K = torch.zeros(m, 1)
        for i = 1, m do
            for j = i, m do
                K[i][j] = kernelFunction.func(X[i]:t(), X[j]:t())
                K[j][i] = K[i][j] --the matrix is symmetric
            end
        end
    end

    -- Train
    io.write('Training ...')
    io.flush()
    local num_changed_alphas
    torch.manualSeed(os.time())
    while passes < max_passes do
        num_changed_alphas = 0
        for i = 1, m do
            -- Calculate Ei = f(x(i)) - y(i) using (2).
            -- E(i) = b + sum (X(i, :) * (repmat(alphas.*Y,1,n).*X):t()) - Y(i)
            E[i] = b + (torch.cmul(torch.cmul(alphas, Y), torch.reshape(K[{ {}, i }], K:size(2)))):sum() - Y[i][1]
            if (Y[i][1] * E[i][1] < -tol and alphas[i][1] < C) or (Y[i][1] * E[i][1] > tol and alphas[i][1] > 0) then
                -- In practice, there are many heuristics one can use to select
                -- the i and j. In this simplified code, we select them randomly.
                local j = math.ceil(m * torch.uniform())
                while j == i do
                    -- Make sure i \neq j
                    j = math.ceil(m * torch.uniform())
                end

                -- Calculate Ej = f(x(j)) - y(j) using (2).
                E[j] = b + (torch.cmul(torch.cmul(alphas, Y), torch.reshape(K[{ {}, j }], K:size(2)))):sum() - Y[j]

                -- Save old alphas
                local alpha_i_old = alphas[i][1]
                local alpha_j_old = alphas[j][1]

                -- Compute L and H by (10) or (11).
                if Y[i][1] == Y[j][1] then
                    L = math.max(0, alphas[j][1] + alphas[i][1] - C)
                    H = math.min(C, alphas[j][1] + alphas[i][1])
                else
                    L = math.max(0, alphas[j][1] - alphas[i][1])
                    H = math.min(C, C + alphas[j][1] - alphas[i][1])
                end

                if L ~= H then
                    -- Compute eta by (14).
                    eta = 2 * K[{ i, j }] - K[{ i, i }] - K[{ j, j }]
                    if eta < 0 then
                        -- Compute and clip new value for alpha j using (12) and (15).
                        alphas[j] = alphas[j][1] - (Y[j][1] * (E[i][1] - E[j][1])) / eta

                        -- Clip
                        alphas[j] = math.min(H, alphas[j][1])
                        alphas[j] = math.max(L, alphas[j][1])

                        -- Check if change in alpha is significant
                        if math.abs(alphas[j][1] - alpha_j_old) < tol then
                            -- continue to next i.
                            -- replace anyway
                            alphas[j] = alpha_j_old
                        else
                            -- Determine value for alpha i using (16).
                            alphas[i] = alphas[i][1] + Y[i][1] * Y[j][1] * (alpha_j_old - alphas[j][1])

                            -- Compute b1 and b2 using (17) and (18) respectively.
                            local b1 = b - E[i][1] - Y[i][1] * (alphas[i][1] - alpha_i_old) * K[i][j] - Y[j][1] * (alphas[i][1] - alpha_i_old) * K[i][j]
                            local b2 = b - E[j][1] - Y[i][1] * (alphas[i][1] - alpha_i_old) * K[i][j] - Y[j][1] * (alphas[i][1] - alpha_i_old) * K[j][j]

                            -- Compute b by (19).
                            if 0 < alphas[i][1] and alphas[i][1] < C then
                                b = b1
                            elseif 0 < alphas[j][1] and alphas[j][1] < C then
                                b = b2
                            else
                                b = (b1 + b2) / 2
                            end

                            num_changed_alphas = num_changed_alphas + 1
                        end
                    end
                end
            end
        end

        if num_changed_alphas == 0 then
            passes = passes + 1
        else
            passes = 0
        end

        io.write('.')
        io.flush()
    end
    print(' Done!')

    -- Save the model
    local idx = alphas:gt(0)
    local oriColSize = X:size(2)
    local idxX = idx:expand(idx:size(1), oriColSize)
    local model = {}
    model.X = X[idxX]
    if model.X:dim() == 0 then
        return nil
    end
    model.X = torch.reshape(model.X, model.X:size(1) / oriColSize, oriColSize)
    model.y = Y[idx]
    model.kernelFunction = kernelFunction
    model.b = b
    model.alphas = alphas[idx]
    model.w = (torch.cmul(alphas, Y):t() * X):t()

    return model
end